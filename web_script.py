from googleapiclient.discovery import build
import requests

# Google API å¯†é’¥å’Œæœç´¢å¼•æ“ ID
API_KEY = "AIzaSyDmfPU0aEM3-WgFQRBWUCtvvhIt2NWNvEs"
CSE_ID = "548acf14d5a484725"

# DeepSeek API é…ç½®
OLLAMA_URL = "http://192.168.0.50:11434/api/chat"
MODEL_NAME = "deepseek-r1:32b"
HEADERS = {"Content-Type": "application/json"}

# è®¾ç½® Google æœç´¢ API
def google_search(query, num_results=10):
    service = build("customsearch", "v1", developerKey=API_KEY)
    res = service.cse().list(q=query, cx=CSE_ID, num=num_results).execute()
    return [item['link'] for item in res['items']]  # è¿”å›æ¯æ¡ç»“æœçš„ URL

# å°†æœç´¢ç»“æœå‘é€ç»™ AI
def ask_deepseek_about(text, user_question):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ ¹æ®ç½‘ç»œå†…å®¹å›ç­”é—®é¢˜çš„ AI åŠ©æ‰‹"},
        {"role": "user", "content": f"è¯·å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{user_question}"},
        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ç›¸å…³å†…å®¹ï¼š\n{text}"}
    ]
    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "stream": False
    }
    res = requests.post(OLLAMA_URL, json=data, headers=HEADERS)
    return res.json()["message"]["content"]

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    from bs4 import BeautifulSoup

    def fetch_webpage_text(url):
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
            }
            res = requests.get(url, headers=headers, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            for tag in soup(["script", "style", "noscript"]):
                tag.decompose()
            text = soup.get_text(separator='\n')
            return '\n'.join(line.strip() for line in text.splitlines() if line.strip())[:2000]
        except Exception as e:
            print(f"âŒ æ— æ³•æŠ“å– {url}ï¼š{e}")
            return ""

    query = input("è¯·è¾“å…¥æœç´¢å†…å®¹ï¼š")
    search_urls = google_search(query)

    print("\nGoogle æœç´¢ç»“æœï¼š")
    contents = []
    for idx, url in enumerate(search_urls, 1):
        print(f"{idx}. {url}")
        content = fetch_webpage_text(url)
        if content:
            contents.append(content)

    combined_results = "\n\n".join(contents)

    user_question = input("\nä½ æƒ³é—®è¿™ç¯‡æ–‡ç« ä»€ä¹ˆï¼Ÿ\n")
    reply = ask_deepseek_about(combined_results, user_question)

    print("\nğŸ¤– AI å›å¤ï¼š\n")
    print(reply)