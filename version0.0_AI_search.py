import requests
from bs4 import BeautifulSoup
# from googleapiclient.discovery import build
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import re
# === é…ç½® ===
OLLAMA_URL = "http://192.168.0.50:11434/api/chat"
MODEL_NAME = "deepseek-r1:32b"
HEADERS = {"Content-Type": "application/json"}

# GOOGLE_API_KEY = "AIzaSyDmfPU0aEM3-WgFQRBWUCtvvhIt2NWNvEs"  # è¯·æ›¿æ¢
# GOOGLE_CSE_ID = "548acf14d5a484725"  # è¯·æ›¿æ¢

def ask_deepseek(messages, include_datetime=False):
    if include_datetime:
        from datetime import datetime
        now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        messages.insert(0, {
            "role": "system",
            "content": f"å½“å‰æ—¥æœŸå’Œæ—¶é—´æ˜¯ï¼š{now}"
        })
    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "stream": False,
        "temperature": 0.2      
    }
    res = requests.post(OLLAMA_URL, json=data, headers=HEADERS)
    res.raise_for_status()
    return res.json()["message"]["content"]

def generate_search_keywords(question):
    messages = [
        {"role": "system", "content": '''
        ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ï¼Œæ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆåˆé€‚çš„ Google æœç´¢å…³é”®è¯
        é»˜è®¤æƒ…å†µä¸‹Market æŒ‡çš„æ˜¯ç¾å›½å¸‚åœº.
        ä½œä¸ºä¸€ä¸ªä¸¥è°¨çš„åŠ©æ‰‹,ä½ ä¼šå°½é‡åœ¨æ¯ä¸€ä¸ªè§‚ç‚¹åé¢é™„ä¸Šæœ€å…³é”®çš„æ•°æ®æ¥æº.
         '''},
        {"role": "user", "content": f"è¯·æ ¹æ®è¿™ä¸ªé—®é¢˜ç”Ÿæˆå‡ ä¸ªæœ‰ç”¨çš„ Google æœç´¢å…³é”®è¯,æœç´¢å°½é‡ç”¨è‹±æ–‡è·å–ï¼š\n\n{question},è¾“å‡ºç»“æœåªåŒ…å«å…³é”®è¯,ä¸€ä¸ªå…¶ä»–å¤šä½™çš„å­—éƒ½ä¸è¦æœ‰"}
    ]
    result = ask_deepseek(messages, include_datetime=True)
    ### clearn think
    think_match = re.search(r'<think>(.*?)</think>', result, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else None
    cleaned_reply = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL).strip()

    # è¿‡æ»¤æ‰åŒ…å« think æ ‡ç­¾çš„å†…å®¹ï¼Œç¡®ä¿åªè¿”å›å…³é”®è¯
    keywords = [re.sub(r'^\d+[\.\)]\s*', '', kw).translate(str.maketrans('', '', '*"\'')) for kw in cleaned_reply.splitlines() if kw.strip()]
    return keywords

def determine_search_need(question):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåˆ¤æ–­ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­ä¸€ä¸ªé—®é¢˜æ˜¯å¦éœ€è¦æœç´¢å¼•æ“è·å–ç­”æ¡ˆã€‚å¦‚æœç”¨æˆ·é—®é¢˜éœ€è¦å®æ—¶ä¿¡æ¯ã€æ•°æ®æ›´æ–°æˆ–ç‰¹å®šç½‘é¡µä¿¡æ¯ï¼Œè¯·å›ç­” YESï¼Œå¦åˆ™å›ç­” NOã€‚åªè¾“å‡º YES æˆ– NOã€‚"},
        {"role": "user", "content": f"{question}"}
    ]
    result = ask_deepseek(messages, include_datetime=True).strip()
    # æ¸…ç† think æ ‡ç­¾
    think_match = re.search(r'<think>(.*?)</think>', result, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else None
    cleaned_reply = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL).strip()


    print("befoire clearn think:", result)
    print( "determine_search_need:", cleaned_reply)
    return cleaned_reply == "YES"

def bing_search(query, max_results=3):
    options = Options()
    options.add_argument("--headless")  # ç•™ç©ºä»¥æ˜¾ç¤ºçª—å£
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(options=options)

    driver.get(f"https://www.bing.com/search?q={query}")
    time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ

    results = driver.find_elements(By.CSS_SELECTOR, 'li.b_algo h2 a')  # ä¿®æ”¹ CSS é€‰æ‹©å™¨
    urls = []
    for result in results:
        href = result.get_attribute("href")
        if href and href.startswith("http"):
            urls.append(href)
        if len(urls) >= max_results:
            break

    driver.quit()
    return urls

def fetch_webpage_text(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        text = soup.get_text(separator='\n')
        return '\n'.join(line.strip() for line in text.splitlines() if line.strip())[:2000]
    except Exception as e:
        print(f"âŒ æ— æ³•æŠ“å– {url}ï¼š{e}")
        return ""

def summarize_with_deepseek(text_blocks, original_question):
    messages = [
        {"role": "system", "content": '''
         ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚ä½ åªèƒ½åŸºäºæä¾›çš„æ–‡æœ¬å’Œå…¬å¼€äº‹å®ä½œç­”ï¼Œä¸å…è®¸å‡è®¾æˆ–çŒœæµ‹.
         ä½ æœ‰è‰¯å¥½çš„ä¹ æƒ¯,ä¼šåœ¨é‡è¦çš„è§‚ç‚¹åé¢é™„ä¸Šæœ€ä¸»è¦çš„æ•°æ®æ¥æºçš„é“¾æ¥,ä¾›ç”¨æˆ·äº¤å‰éªŒè¯.
         '''},
        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ä¸€äº›ç½‘é¡µå†…å®¹ï¼Œè¯·åŸºäºè¿™äº›ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{original_question}\n\n{text_blocks}"}
    ]
    return ask_deepseek(messages, include_datetime=True)

if __name__ == "__main__":
    user_question = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š\n")
    if determine_search_need(user_question):
        keywords = generate_search_keywords(user_question)
        print("\nğŸ” AI å»ºè®®æœç´¢å…³é”®è¯ï¼š")
        for kw in keywords:
            print(f"- {kw}")
    else:
        print("\nâ„¹ï¸ AI åˆ¤æ–­æ­¤é—®é¢˜ä¸éœ€è¦æœç´¢ï¼Œç›´æ¥å›ç­”ÃŸ...")
        keywords = []

    urls = []
    if keywords:
        for kw in keywords:
            urls += bing_search(kw)

    print("\nğŸ“„ æœç´¢åˆ°çš„ç½‘é¡µï¼š%d"%len(urls))
    for url in urls:
        print(url)

    contents = []
    for i, url in enumerate(urls, 1):
        print(f"ğŸ•¸ï¸ æ­£åœ¨æŠ“å–ç¬¬ {i}/{len(urls)} ä¸ªç½‘é¡µï¼š{url}")
        content = fetch_webpage_text(url)
        if content:
            contents.append(content)
    combined_text = "\n\n".join(contents)

    print("\nğŸ¤– æ­£åœ¨æ€»ç»“ä¿¡æ¯ï¼Œè¯·ç¨å€™...\n")
    summary = summarize_with_deepseek(combined_text, user_question)
    print("âœ… æ€»ç»“ç»“æœï¼š\n")
    print(summary)
    #print(bing_search("python"))
