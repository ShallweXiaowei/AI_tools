import requests
from bs4 import BeautifulSoup
# from googleapiclient.discovery import build
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import re
from push_bark import push_bark
import json
import os



def remove_think(res):
    # æ¸…ç† think æ ‡ç­¾
    think_match = re.search(r'<think>(.*?)</think>', res, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else None
    cleaned_reply = re.sub(r'<think>.*?</think>', '', res, flags=re.DOTALL).strip()
    return cleaned_reply
    #print("befoire clearn think:", res)
# === é…ç½® ===
OLLAMA_URL = "http://localhost:11434/api/chat"
#MODEL_NAME = "deepseek-r1:14b"
#MODEL_NAME = "gemma3:27b"
#MODEL_NAME = "deepseek-r1:14b"
MODEL_NAME = "deepseek-r1:14b-qwen-distill-q8_0"

HEADERS = {"Content-Type": "application/json"}

# GOOGLE_API_KEY = "AIzaSyDmfPU0aEM3-WgFQRBWUCtvvhIt2NWNvEs"  # è¯·æ›¿æ¢
# GOOGLE_CSE_ID = "548acf14d5a484725"  # è¯·æ›¿æ¢

def ask_deepseek(messages, include_datetime=True):
    if include_datetime:
        from datetime import datetime
        now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        messages.insert(0, {
            "role": "system",
            "content": f"å½“å‰æ—¥æœŸå’Œæ—¶é—´æ˜¯ï¼š{now}"
        })
    print(f"å½“å‰æ—¥æœŸå’Œæ—¶é—´æ˜¯ï¼š{now}")
    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "stream": False,
        "temperature": 0.4,
        #"options": {"num_gpu":1}
    }
    res = requests.post(OLLAMA_URL, json=data, headers=HEADERS)
    res.raise_for_status()
    return res.json()["message"]["content"]

def generate_search_keywords(question):
    messages = [
        {"role": "system", "content": '''
        ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ï¼Œæ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆåˆé€‚çš„ Google æœç´¢å…³é”®è¯
        æ¶‰åŠåˆ°ç¾å›½çš„å…¬å¸,æ–°é—»,è´¢æŠ¥,è‚¡å¸‚ç­‰é—®é¢˜,ç”¨è‹±æ–‡å…³é”®è¯æœç´¢
        å¦‚æœé—®é¢˜åŒ…å«'æœ€è¿‘',å°±éœ€è¦ç”¨æœç´¢å¼•æ“äº†
         '''},
        {"role": "user", "content": f"è¯·æ ¹æ®è¿™ä¸ªé—®é¢˜ç”Ÿæˆå‡ ä¸ªæœ‰ç”¨çš„ Google æœç´¢å…³é”®è¯ï¼š\n\n{question},è¾“å‡ºç»“æœåªåŒ…å«å…³é”®è¯.å…³é”®è¯å°½é‡ç®€çŸ­ä»¥ä¿è¯æ¶µç›–åˆ°æµé‡å¤§çš„ç½‘ç«™,æœ€å¥½ä¸è¦è¶…è¿‡10ä¸ª.å½“é—®é¢˜æ¶‰åŠåˆ°è‚¡å¸‚,å…¬å¸è´¢æŠ¥ç­‰ä¿¡æ¯,ç”¨è‹±æ–‡å…³é”®è¯æœç´¢"}
    ]
    result = ask_deepseek(messages, include_datetime=True)
    print("############Generating key words:", result)
    ### clearn think
    think_match = re.search(r'<think>(.*?)</think>', result, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else None
    cleaned_reply = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL).strip()

    # è¿‡æ»¤æ‰åŒ…å« think æ ‡ç­¾çš„å†…å®¹ï¼Œç¡®ä¿åªè¿”å›å…³é”®è¯
    lines = cleaned_reply.replace("\n", ",").split(",")
    keywords = [kw.strip().translate(str.maketrans('', '', '*"\'')) for kw in lines if kw.strip()]
    return keywords

def determine_search_need(question):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåˆ¤æ–­ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­ä¸€ä¸ªé—®é¢˜æ˜¯å¦éœ€è¦æœç´¢å¼•æ“è·å–ç­”æ¡ˆã€‚å¦‚æœç”¨æˆ·é—®é¢˜éœ€è¦å®æ—¶ä¿¡æ¯ã€æ•°æ®æ›´æ–°æˆ–ç‰¹å®šç½‘é¡µä¿¡æ¯ï¼Œè¯·å›ç­” YESï¼Œå¦åˆ™å›ç­” NOã€‚åªè¾“å‡º YES æˆ– NOã€‚"},
        {"role": "user", "content": f"{question}"}
    ]
    result = ask_deepseek(messages, include_datetime=True).strip()
    # æ¸…ç† think æ ‡ç­¾
    think_match = re.search(r'<think>(.*?)</think>', result, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else None
    cleaned_reply = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL).strip()


    print("befoire clearn think:", result)
    print( "determine_search_need:", cleaned_reply)
    return cleaned_reply == "YES"

def bing_search(query, max_results=2):
    options = Options()
    options.add_argument("--headless")  # ç•™ç©ºä»¥æ˜¾ç¤ºçª—å£
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(options=options)

    driver.get(f"https://www.bing.com/search?q={query}")
    time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ

    results = driver.find_elements(By.CSS_SELECTOR, 'li.b_algo h2 a')  # ä¿®æ”¹ CSS é€‰æ‹©å™¨
    urls = []
    for result in results:
        href = result.get_attribute("href")
        if href and href.startswith("http"):
            urls.append(href)
        if len(urls) >= max_results:
            break

    driver.quit()
    return urls

def fetch_webpage_text(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        text = soup.get_text(separator='\n')
        #return '\n'.join(line.strip() for line in text.splitlines() if line.strip())[:3000]
        return '\n'.join(line.strip() for line in text.splitlines() if line.strip())
    except Exception as e:
        print(f"âŒ æ— æ³•æŠ“å– {url}ï¼š{e}")
        return ""

def summarize_each_page(text, url, original_question):
    messages = [
        {"role": "system", "content": '''
        ä½ æ˜¯ä¸€ä¸ªç½‘é¡µæ‘˜è¦åŠ©æ‰‹ï¼Œä¼šå¯¹å•ä¸€ç½‘é¡µå†…å®¹è¿›è¡Œæ€»ç»“, ç›®æ ‡æ˜¯ä¸ºäº†æ€»ç»“ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
        å¦‚æœ‰é‡è¦ä¿¡æ¯ï¼Œè¯·ä¿ç•™ï¼Œå¹¶é™„ä¸Šè¯¥ç½‘é¡µæ¥æºé“¾æ¥ã€‚
        '''},
        {"role": "user", "content": f"ç½‘é¡µå†…å®¹æ¥è‡ªï¼š{url}\n\n{text}"},
        {"role": "user", "content": f"è¯·æ€»ç»“è¿™ç¯‡ç½‘é¡µçš„ä¸»è¦å†…å®¹,ä»¥ä¾¿å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\né—®é¢˜æ˜¯ï¼š{original_question}"}
    ]

    result = ask_deepseek(messages, include_datetime=True)
    think_match = re.search(r'<think>(.*?)</think>', result, re.DOTALL)
    think_content = think_match.group(1).strip() if think_match else None
    cleaned_reply = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL).strip()
    print("summarize_each_page:", cleaned_reply)
    return cleaned_reply

def summarize_with_deepseek(text_blocks, original_question):
    messages = [
        {"role": "system", "content": '''
         ç”¨æˆ·å–œæ¬¢è¯¦ç»†çš„ä¿¡æ¯,ä½œä¸ºä¸€ä¸ªåŠ©æ‰‹,ä¼šè®©å›ç­”å°½é‡è¯¦ç»†, æä¾›å°½å¯èƒ½å¤šçš„ä¿¡æ¯
         ä½ ä¼šå°½é‡ç”¨ä¸­æ–‡å›ç­”é—®é¢˜.
         ä½ ä¼šå°½é‡åœ¨é‡è¦çš„åœ°æ–¹åŠ ä¸Šæ•°æ®æºé“¾æ¥
         '''},
        {"role": "user", "content": f"è¯·å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{original_question}\nä½ çš„å›ç­”ä¸ç”¨ä¸¥æ ¼å±€é™è¿™ä¸ªé—®é¢˜,å¯ä»¥æ£æµ‹ç”¨æˆ·ç›®çš„,æä¾›æ›´å¤šä¿¡æ¯"},
        {"role": "user", "content": f"ä»¥ä¸‹æ˜¯ä¸€äº›ç½‘é¡µå†…å®¹ä»¥ä¾›å‚è€ƒ \n\n{text_blocks}"}
    ]
    return ask_deepseek(messages, include_datetime=True)

def safe_filename(name):
    return "".join(c for c in name if c.isalnum() or c in "._- ")[:50]

def save_session(user_question, urls, combined_summary_text, final_summary, save_dir="saved_sessions"):
    from datetime import datetime
    os.makedirs(save_dir, exist_ok=True)
    timestamp = int(time.time())
    date_str = datetime.now().strftime("%Y-%m-%d")
    safe_name = safe_filename(user_question)
    filename = os.path.join(save_dir, f"{timestamp}_{date_str}_{safe_name}.json")
    session = {
        "user_question": user_question,
        "urls": urls,
        "combined_summary": combined_summary_text,
        "final_summary": final_summary
    }
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(session, f, ensure_ascii=False, indent=2)
    print(f"âœ… æœ¬æ¬¡æœç´¢è®°å½•å·²ä¿å­˜åˆ° {filename}")

def load_session(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        session = json.load(f)
    return session

def list_sessions(save_dir="saved_sessions"):
    if not os.path.exists(save_dir):
        print("âš ï¸ æš‚æ— ä¿å­˜è®°å½•")
        return []
    files = sorted(os.listdir(save_dir), key=lambda x: os.path.getmtime(os.path.join(save_dir, x)), reverse=True)
    for i, f in enumerate(files):
        print(f"{i+1}: {f}")
    return files

def search_sessions(keyword, save_dir="saved_sessions"):
    if not os.path.exists(save_dir):
        print("âš ï¸ æš‚æ— ä¿å­˜è®°å½•")
        return []
    files = sorted(os.listdir(save_dir), key=lambda x: os.path.getmtime(os.path.join(save_dir, x)), reverse=True)
    matched = []
    for i, f in enumerate(files):
        path = os.path.join(save_dir, f)
        with open(path, "r", encoding="utf-8") as fp:
            session = json.load(fp)
            if keyword in session.get("user_question", "") or keyword in session.get("final_summary", ""):
                matched.append((i+1, f))
    if matched:
        for idx, name in matched:
            print(f"{idx}: {name}")
    else:
        print(f"æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„å†å²è®°å½•ã€‚")
    return matched

if __name__ == "__main__":
    while True:
        user_input = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæˆ–è¾“å…¥ /list /load n /search å…³é”®è¯ /exitï¼‰ï¼š\n").strip()
        
        if user_input == "/exit":
            break
        elif user_input == "/list":
            list_sessions()
        elif user_input.startswith("/load"):
            parts = user_input.split()
            if len(parts) < 2:
                print("â— ç”¨æ³•ï¼š/load ç¼–å· [ç¼–å·2 ç¼–å·3...]")
                continue
            indices = [int(x) - 1 for x in parts[1:]]
            files = list_sessions()

            combined_text = ""
            for idx in indices:
                if 0 <= idx < len(files):
                    chosen_file = os.path.join("saved_sessions", files[idx])
                    session = load_session(chosen_file)
                    combined_text += session['combined_summary'] + "\n\n"
                else:
                    print(f"âš ï¸ å¿½ç•¥æ— æ•ˆç¼–å· {idx+1}")

            if not combined_text:
                print("âš ï¸ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å†…å®¹ã€‚")
                continue

            print(f"\nğŸ“„ å·²åŠ è½½ {len(indices)} ä¸ªå†å²æ€»ç»“ã€‚")
            new_question = input("è¯·è¾“å…¥æƒ³åœ¨è¿™äº›èµ„æ–™åŸºç¡€ä¸Šç»§ç»­æé—®çš„é—®é¢˜ï¼š\n")

            followup = summarize_with_deepseek(combined_text, new_question)
            print(f"\nâœ… AIå›ç­”ï¼š\n{followup}")
            #push_bark(title="AIåˆ†æå®Œæˆ", body=remove_think(followup))
        elif user_input.startswith("/search"):
            keyword = user_input[len("/search"):].strip()
            if not keyword:
                print("â— ç”¨æ³•ï¼š/search å…³é”®è¯")
            else:
                search_sessions(keyword)
        else:
            user_question = user_input
            if determine_search_need(user_question):
                keywords = generate_search_keywords(user_question)
                print("\nğŸ” AI å»ºè®®æœç´¢å…³é”®è¯ï¼š")
                for kw in keywords:
                    print(f"- {kw}")
            else:
                print("\nâ„¹ï¸ AI åˆ¤æ–­æ­¤é—®é¢˜ä¸éœ€è¦æœç´¢ï¼Œç›´æ¥å›ç­”...")
                keywords = []

            urls = []
            if keywords:
                for kw in keywords:
                    urls += bing_search(kw)
            urls = list(dict.fromkeys(urls))

            print("\nğŸ“„ æœç´¢åˆ°çš„ç½‘é¡µï¼š%d" % len(urls))
            for url in urls:
                print(url)

            summaries = []
            for i, url in enumerate(urls, 1):
                print(f"ğŸ•¸ï¸ æ­£åœ¨æŠ“å–ç¬¬ {i}/{len(urls)} ä¸ªç½‘é¡µï¼š{url}")
                content = fetch_webpage_text(url)
                if content:
                    summary = summarize_each_page(content, url, user_question)
                    summaries.append(f"[{url}]\n{summary}")

            combined_summary_text = "\n\n".join(summaries)

            print("\nğŸ¤– æ­£åœ¨æ€»ç»“ä¿¡æ¯ï¼Œè¯·ç¨å€™...\n")
            summary = summarize_with_deepseek(combined_summary_text, user_question)
            print("âœ… æ€»ç»“ç»“æœï¼š\n")
            print(summary)

            #push_bark(title="AIåˆ†æå®Œæˆ", body=remove_think(summary))
            save_session(user_question, urls, combined_summary_text, summary)
